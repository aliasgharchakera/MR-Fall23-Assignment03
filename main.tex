\documentclass[answers]{exam}

\usepackage{amsmath, amsfonts, amssymb}
\usepackage{geometry}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{listings}
% \usepackage{subfig}
\usepackage{float}

\lstset{
    basicstyle=\ttfamily,
    columns=fullflexible,
    frame=single,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}

% Header and footer.
\pagestyle{headandfoot}
\runningheadrule
\runningfootrule
\runningheader{EE/CE 468/468 Mobile Robotics}{Homework 3}{Fall 2023}
\runningfooter{}{Page \thepage\ of \numpages}{}
\firstpageheader{}{}{}

\boxedpoints
\printanswers

\newcommand{\uvec}[1]{\boldsymbol{\hat{\textbf{#1}}}}
\newcommand\union\cup 
\newcommand\inter\cap
\newcommand\ul\underline
\newcommand\ol\overline

\title{Assignment 3\\ EE/CE 468/468 Mobile Robotics\\ Habib University -- Fall 2023}
\author{Ali Asghar Yousuf \\ Muhammad Azeem Haider }
\date{\today}

\begin{document}
\maketitle

\begin{questions}
    \question[20]
    Consider a robot that lives in a 1-D coordinate system. Its location will be denoted by \(x\), its velocity by \(\dot{x}\), and its acceleration by \(\ddot{x}\). Suppose we can only control the acceleration \(\ddot{x}\). Making use of equations of motion from school physics, write the discrete-time motion model for this system. Assume that the acceleration \(\ddot{x}\) is a sum of a commanded acceleration and a zero-mean noise term with variance \(\sigma^2\) and assume that the actual acceleration remains constant in an interval \(\Delta t\).
    \begin{parts}
        \part Find the uncertainty/covariance in the pose \((x, \dot{x})\) after one time step. Are the two correlated?
        \begin{solution}
    \textbf{Solution for Part A:}
    
    Let's think of our robot moving in a straight line. We know its position \( x \), velocity \( \dot{x} \), and we control its acceleration \( \ddot{x} \), which has some unpredictable part (like random bumps on the road). We represent this uncertainty in acceleration as a zero-mean noise with variance \( \sigma^2 \).

    Now, to update the robot's position and velocity, we use basic physics:
    \begin{itemize}
        \item The new velocity \( \dot{x}_{t+1} \) is the old velocity \( \dot{x}_t \) plus acceleration times the time interval \( \Delta t \).
        \item The new position \( x_{t+1} \) is the old position \( x_t \) plus the old velocity times \( \Delta t \) plus half the acceleration times \( \Delta t \) squared.
    \end{itemize}
    
    For the uncertainty part, it's like predicting the weather. We start with what we know (the current state of the robot) and then add in our "best guess" for how things change (our control input plus the random noise). The uncertainty in the robot's position and velocity after one step can be captured in a matrix called the covariance matrix. This matrix tells us how confident we are about the robot's position and velocity and whether there's any link (correlation) between them.

    In short, this approach shows that even if we start off certain about the robot's position and velocity, the unpredictable part of the acceleration introduces some doubt, making the position and velocity interrelated (correlated) after one time step.
\end{solution}

        
        \part Suppose we control this robot with a commanded acceleration sequence \(a_1, a_2, a_3, \ldots\) for \(T\) time intervals. Will the final location \(x\) and the final velocity \(\dot{x}\) be correlated for some large value of \(T\)?
        \begin{solution}
            \textbf{Solution for Part B:}
            
            Given the discrete-time dynamical system with state vector at time step \( t \) as \( \mathbf{x}_t \) and control input as the acceleration \( u_t \) with additive Gaussian noise \( \epsilon_t \), the system can be described as:
            
            \begin{itemize}
                \item The control input noise \( \epsilon_t \) is Gaussian distributed with zero mean and variance \( \sigma^2 \), that is \( \epsilon_t \sim \mathcal{N}(0, \sigma^2) \).
                \item The system follows a linear state-space model with the state transition matrix \( \mathbf{A} \) and control input matrix \( \mathbf{B} \).
            \end{itemize}
            
            The state update equation is:
            \[ \mathbf{x}_{t+1} = \mathbf{A} \mathbf{x}_t + \mathbf{B} (u_t + \epsilon_t) \]
            where
            \[ \mathbf{A} = \begin{bmatrix} 1 & \Delta t \\ 0 & 1 \end{bmatrix}, \quad \mathbf{B} = \begin{bmatrix} \frac{1}{2} (\Delta t)^2 \\ \Delta t \end{bmatrix}, \quad \mathbf{x}_t = \begin{bmatrix} x_t \\ \dot{x}_t \end{bmatrix}. \]
            
            After \( T \) time intervals, the final state \( \mathbf{x}_T \) can be expressed as:
            \[ \mathbf{x}_T = \mathbf{A}^T \mathbf{x}_0 + \sum_{t=0}^{T-1} \mathbf{A}^{T-t-1} \mathbf{B} (u_t + \epsilon_t). \]
            
            The covariance matrix of the final state \( \mathbf{x}_T \), which encapsulates the accumulated uncertainty from the process noise, is given by:
            \[ \mathbf{\Sigma}_T = \sum_{t=0}^{T-1} \mathbf{A}^{T-t-1} \mathbf{B} \mathbf{Q} \mathbf{B}^\top (\mathbf{A}^{T-t-1})^\top, \]
            where \( \mathbf{Q} \) is the covariance matrix of the process noise \( \epsilon_t \).
            
            The non-zero off-diagonal terms of the covariance matrix \( \mathbf{\Sigma}_T \) indicate a correlation between the final location and velocity. This correlation results from the control input noise, which affects both velocity and position over time due to the system's dynamics.
        \end{solution}
                   
    \end{parts}

    \question[20]
    Suppose we have a mobile robot operating in a planar environment. Its state is its \(x\)-\(y\) location and its global heading direction \(\theta\). Suppose we know \(x\) and \(y\) with high certainty but the orientation \(\theta\) is unknown. This is reflected in our initial estimate:
    \[
    \hat{x}_0 = \begin{bmatrix} 0 \\ 0 \\ 0 \end{bmatrix}, \quad
    \Sigma_0 = \begin{bmatrix} 0.01 & 0 & 0 \\ 0 & 0.01 & 0 \\ 0 & 0 & 10000 \end{bmatrix}.
    \]
    \begin{parts}
        \part Assume that the robot moves flawlessly without any noise. We'll consider the simple case when the robot's heading is not being controlled i.e. \(\omega = 0\). Observations of the robot are made at discrete points in time and consist of the robot's distance from the origin \(d\) and the bearing \(\theta\) measured from the origin. Assume that the noise associated with these two measurements are independent. Develop a Kalman Filter that maintains an estimate of the robot’s state.
        \begin{solution}
        \end{solution}

        \part The location of the robot is a random vector. Draw 1000 samples of the initial state from a Gaussian distribution of the stated mean and covariance, propagate each initial state sample according to the motion equation and plot the samples of the \(x\)-\(y\) state only at time 1 in MATLAB. Assume that distance covered in one time step is 1.
        \begin{solution}
        \end{solution}

        \part Use the prediction step of the EKF to make a prediction about the state at time 1 and its corresponding covariance. Plot the uncertainty ellipse of a Gaussian with mean equal to \(\bar{x}_1\) and covariance \(\bar{\Sigma}_1\) on the same plot as (a) and compare and comment on the two.
        \begin{solution}
        \end{solution}

        \part Now incorporate a noisy measurement i.e. \(z = d + \epsilon\) where \(\epsilon\) is zero-mean with covariance 0.01. Again draw the uncertainty ellipse on the same plot after incorporating the measurement.
        \begin{solution}
        \end{solution}

        \part What would have been your estimate for the \(x\)-\(y\) at time 1 considering (a)? What would be your comments about the estimate provided by the EKF? What would have happened if the initial orientation were known but we were uncertain about the \(y\) coordinate?
        \begin{solution}
        \end{solution}
    \end{parts}

    \question[20]
    Suppose we live at a place where days are either sunny, cloudy, or rainy. The weather tomorrow is determined solely by the weather today (it’s a Markov Chain) and is captured by the following state transition probabilities:
    \begin{center}
    \textbf{Today's Weather} \\
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{} & \textbf{Sunny} & \textbf{Cloudy} & \textbf{Rainy} \\ \hline
    \textbf{Sunny} & 0.8 & 0.2 & 0 \\ \hline
    \textbf{Cloudy} & 0.4 & 0.4 & 0.2 \\ \hline 
    \textbf{Rainy} & 0.2 & 0.6 & 0.2 \\ \hline
    \end{tabular}
    \end{center}
    Suppose that we cannot observe the weather directly but instead rely on a sensor. Our sensor is noisy. The measurements are governed by the following measurement model:
    \begin{center}
    \textbf{Actual Weather} \\
    \begin{tabular}{|c|c|c|c|}
    \hline
    \textbf{} & \textbf{Sunny} & \textbf{Cloudy} & \textbf{Rainy} \\ \hline
    \textbf{Sunny} & 0.6 & 0.4 & 0 \\ \hline
    \textbf{Cloudy} & 0.3 & 0.7 & 0 \\ \hline
    \textbf{Rainy} & 0 & 0 & 1 \\ \hline
    \end{tabular}
    \end{center}
    \begin{parts}
        \part Suppose Day 1 is sunny (this is known for a fact). At days 2 through 4 the sensor measures sunny, sunny, rainy. For each of the days 2 through 4 what is the most likely weather on that day. Answer the question in two ways: one in which only the data available to the day in question is used and one in hindsight where data from future days is also available.
        
        \begin{solution}
            \textbf{Day 2 $\mid$ only data available to the day in question is used}
            \begin{align*}
                P(x_2 \mid x_1, z_2) &= \eta P(z_2 \mid x_2, x_1) P(x_2 \mid x_1) \\
                &= \eta P(z_2 \mid x_2) P(x_2 \mid x_1) \\
                &= \eta \begin{bmatrix}
                    0.6 \\
                    0.3 \\
                    0 
                \end{bmatrix} \cdot \begin{bmatrix}
                    0.8 \\
                    0.2 \\
                    0
                \end{bmatrix} \\
                &= \eta \begin{bmatrix}
                    0.48 \\
                    0.06 \\
                    0
                \end{bmatrix} \\
                &= \eta \left( 0.48 + 0.06 + 0 \right) \begin{bmatrix}
                    0.48 \\
                    0.06 \\
                    0
                \end{bmatrix} \\
                &= \frac{1}{0.54} \begin{bmatrix}
                    0.48 \\
                    0.06 \\
                    0
                \end{bmatrix} \\
                &= \begin{bmatrix}
                    8/9 \\
                    1/9 \\
                    0
                \end{bmatrix}
            \end{align*}
            Here, \(\eta = \frac{1}{0.54}\) is the normalizing constant. \\
            Therefore, the most likely weather on Day 2 is \textit{sunny} given that we are only using the data available to us.
            
            \textbf{Day 3 $\mid$ only data available to the day in question is used}
            \begin{align*}
                P(x_3 \mid x_2, z_{2:3}) &= \eta P(z_3 \mid x_3, x_1, x_2) P(x_3 \mid x_1, x_2) \\
                &= \eta P(z_3 \mid x_3) \sum_{x_2} P(x_3, x_2 \mid x_1, x_2) \\
                &= \eta P(z_3 \mid x_3) \sum_{x_2} P(x_3 \mid x_2) P(x_2 \mid x_1, x_2)
            \end{align*}
            
            \text{We know:}
            \begin{align*}
                P(x_2 \mid x_1, z_2) &= \begin{bmatrix}
                    8/9 \\
                    1/9 \\
                    0
                \end{bmatrix} \\
                P(z_3 \mid x_3) &= \begin{bmatrix}
                    0.6 \\
                    0.3 \\
                    0
                \end{bmatrix} \\
                \text{We get the following equation:} \\
                P(x_3 \mid x_2, z_{2:3}) &= \eta \begin{bmatrix}
                    0.6 \\
                    0.3 \\
                    0
                \end{bmatrix} \sum_{x_2} P(x_3 \mid x_2) \begin{bmatrix}
                    8/9 \\
                    1/9 \\
                    0
                \end{bmatrix}
            \end{align*}
            
            \text{For Sunny:}
            \begin{align*}
                0.6 \times \left[\begin{bmatrix} 0.8 & 0.4 & 0.2 \end{bmatrix} \cdot \begin{bmatrix}
                    8/9 \\
                    1/9 \\
                    0
                \end{bmatrix}\right] &= \frac{34}{75}
            \end{align*}
            
            \text{For Cloudy:}
            \begin{align*}
                0.3 \times \left[\begin{bmatrix} 0.2 & 0.4 & 0.6 \end{bmatrix} \cdot \begin{bmatrix}
                    8/9 \\
                    1/9 \\
                    0
                \end{bmatrix}\right] &= \frac{1}{15}
            \end{align*}

            \text{For Rainy:}
            \begin{align*}
                0 \times \left[\begin{bmatrix} 0 & 0.2 & 0.2 \end{bmatrix} \cdot \begin{bmatrix}
                    8/9 \\
                    1/9 \\
                    0
                \end{bmatrix}\right] &= 0
            \end{align*}
            \text{Normalizing the matrix to get the accurate probability we get;}

            Sunny $=$ 87.2\% \\
            Cloudy $=$ 12.8\% \\
            Rainy $=$ 0\% \\

            Therefore, once again the most likely weather on Day 3 is sunny given that we are only using the data available to us.

            \textbf{Day 4 $\mid$ only data available to the day in question is used}
            \begin{align*}
                P(x_4 | x_1, x_2;x_4) &= P(x_4 | x_3, x_4) \\
                P(x_4 | x_1, x_2;x_4) &= \eta P(z_4 | x_4)P(x_4 | x_3) \\
                P(x_4 | x_1, x_2;x_4) &= \begin{bmatrix}
                0 \\
                0 \\
                1
                \end{bmatrix}
            \end{align*}

            We know for sure that on the fourth day it will rain if the sensor has predicted it to rain.

            % \newpage
            \textbf{Day 2 $\mid$ where data from future days is also available}
            % \vspace{-5mm}
            \begin{align*}
                P(x_{2:4} | x_1, z_{2:4}) &= \eta P(x_{2:4} | x_1) P(z_{2:4} | x_{2:4}, x_1) \\
                &= \eta P(x_{2} | x_1) P(z_{2:4} | x_2) \\
                &= \eta P(x_{2} | x_1) P(z_{2} | x_2) \sum_{x_3} P(x_3 | x_2, z_2) P(z_{3:4} | x_2) \\
                &= \eta P(x_{2} | x_1) P(z_{2} | x_2) \sum_{x_3} P(x_3 | x_2) P(z_{3:4} | x_3, x_2) \\
                &= \eta P(x_{2} | x_1) P(z_{2} | x_2) \sum_{x_3} P(x_3 | x_2) P(z_3 | x_3) P(z_{4:3} | x_3) \\
                &= \eta P(x_{2} | x_1) P(z_{2} | x_2) \sum_{x_3} P(x_3 | x_2) P(z_3 | x_3) \sum_{x_4} P(x_4 | x_3) P(z_4 | x_4, x_3) \\
                &= \eta P(x_{2} | x_1) P(z_{2} | x_2) \sum_{x_3} P(x_3 | x_2) P(z_3 | x_3) \sum_{x_4} P(x_4 | x_3) P(z_4 | x_3) \\
                &= \eta P(x_{2} | x_1) P(z_{2} | x_2) \sum_{x_3} P(x_3 | x_2) P(z_3 | x_3) \sum_{x_4} P(x_4 | x_3) P(z_4 | x_4) \\ 
            \end{align*}
            % \vspace{-10mm}
            \begin{align*}    
                P(z_4 | x_4) &= \begin{bmatrix}
                0 \\
                0 \\
                1
                \end{bmatrix} \\
                P(z_3 | x_3) &= \begin{bmatrix}
                    0.6 \\
                    0.3 \\
                    0
                \end{bmatrix} \\
                \sum_{x_4} P(x_4 | x_3) P(z_4 | x_4) &= \begin{bmatrix} 0 \\ 0.2 \\ 0.2 \end{bmatrix} \\
                P(z_3 | x_3) \cdot \sum_{x_4} P(x_4 | x_3) P(z_4 | x_4) &= \begin{bmatrix}
                    0.6 \\
                    0.3 \\
                    0
                \end{bmatrix} \cdot \begin{bmatrix} 0 \\ 0.2 \\ 0.2 \end{bmatrix} \\ &= \begin{bmatrix} 0 \\ 0.06 \\ 0 \end{bmatrix} \\
                \sum_{x_3} P(x_3 | x_2) \cdot \begin{bmatrix} 0 \\ 0.06 \\ 0 \end{bmatrix} &= \begin{bmatrix} 0.012 \\ 0.024 \\ 0.036 \end{bmatrix} \\   
                P(x_{2} | x_1) \cdot P(z_{2} | x_2) \cdot \begin{bmatrix} 0.012 \\ 0.024 \\ 0.036 \end{bmatrix} &= \begin{bmatrix} 0.8 \\ 0.2 \\ 0 \end{bmatrix} \cdot \begin{bmatrix} 0.6 \\ 0.3 \\ 0 \end{bmatrix} \cdot \begin{bmatrix} 0.012 \\ 0.024 \\ 0.036 \end{bmatrix} &= \begin{bmatrix} 0.00576 \\ 0.00144 \\ 0 \end{bmatrix} \\
            \end{align*}

            \begin{align*} 
                \intertext{Normalizing the matrix to get the accurate probability we get;} 
                \begin{bmatrix} 0.8 \\ 0.2 \\ 0 \end{bmatrix}               
            \end{align*}
            
            \textbf{Day 3 $\mid$ where data from future days is also available}
            \begin{align*}
                P(x_3 | x_1, z_{2:4}) &= \eta P(x_3 | x_1, z_{2:3})P(z_4 | x_3, x_1, z_{2:3}) \\
                &= \eta P(x_3 | x_2, z_3)P(z_4 | x_3) \\
                &= \eta \begin{bmatrix}
                0.8395 \\
                0.1605 \\
                0
                \end{bmatrix} \cdot \begin{bmatrix}
                0 \\
                0.2 \\
                0
                \end{bmatrix} \\
                &= \begin{bmatrix}
                0 \\
                1 \\
                0
                \end{bmatrix}
            \end{align*}

            \textbf{Day 4 $\mid$ where data from future days is also available}
            \begin{align*}
                P(x_4 | x_1, x_2;x_4) &= P(x_4 | x_3, x_4) \\
                P(x_4 | x_1, x_2;x_4) &= \eta P(z_4 | x_4)P(x_4 | x_3) \\
                P(x_4 | x_1, x_2;x_4) &= \begin{bmatrix}
                0 \\
                0 \\
                1
                \end{bmatrix}
            \end{align*}
            Since we have no future days after Day 4, the probability will remain the same for Day 4. 
        \end{solution}                 
            
        \part Consider the same situation. What is the most likely sequence of weather for Days 2 through 4? What is the probability of the most likely sequence?

        \begin{solution}
            The probability of the sequence of weather is given by
            \begin{align*}
            P(z_{2:4}|x_1, z_{2:4}) &= \eta P(z_{2:4}|x_1, z_{2:4})P(x_{2:4}|x_1) \\
            \intertext{where}
            P(z_{2:4}|x_1) &= P(z_4|x_3)P(z_3|x_2)P(z_2|x_1) \\
            \intertext{and}
            P(x_{2:4}|x_1, z_{2:4}) &= P(z_4|x_4)P(z_3|x_3)P(z_2|x_2)
            \end{align*}

            Hence, the most likely sequence of weather is \textit{sunny, cloudy, rainy} which has \(0.00576\cdot\dfrac{1}{0.00576+0.00144} = 80\%\) of occurring. There is a \(20\%\) probability of \textit{cloudy, cloudy, rainy} and \(0\%\) probability for all other sequences of weather.
        \end{solution}
    \end{parts}

    \question[20]
    \begin{parts}
        \part Complete the incrementalLocalization and all of its subsidiary functions.
        \begin{solution}
        \end{solution}

        \part Comment on the performance of the EKF-based localization after running the simulation for a longer time.
        \begin{solution}
        \end{solution}

        \part Provide an explanation with reference to code on how the measurement uncertainty covariance matrix R is computed from the uncertainty of the lidar.
        \begin{solution}
        \end{solution}

        \part (Bonus) Apply the Unscented Kalman Filter to this problem.
        \begin{solution}
        \end{solution}
    \end{parts}

    \question[20]
    Answer the following questions individually:
    \begin{parts}
        \part How many hours did each of you spend on this homework?
        \begin{solution}
            
        \end{solution}

        \part State each group member's specific contribution to this homework assignment.
        \begin{solution}
        \end{solution}

        \part Do you have any specific advice for students attempting this homework next year?
        \begin{solution}
            \textbf{Azeem} Start the homework early, you may look at the questions and tell yourself yeah these are simple questions, markov chains aint that difficult, and state estimation is not all that challenging but when you actually start doing the questions you realize you know too little and the questions really test your knowledge. Start early, no matter what your mind tells you. In addition, know your concepts, it is essential to know where you are struggling and fix it through this homework and take help from the instructor and your peers. 
        \end{solution}

        \part Provide a self-reflection in the form of a note or a concept map.
        \begin{solution}
            \textbf{Azeem} Attempting questions are always difficult because this is like the hands on practise you get after lectures. I attempted question 3 first, because in my opinion, it looked the easiest and I could not be more wrong. While it made sense once I understood it, making sense to why the markov chains and bayesian estimation works the way it works is difficult. In addition, using state estimation to solve the covariance matrix was also difficult. I am not entirely sure about the correctness of my question 1. 
        \end{solution}
    \end{parts}
\end{questions}

\end{document}